# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qa_AP124Smeu-T5BKJh-EoDq1bTroCHh
"""

import numpy as np  # Numerical computations and linear algebra
import pandas as pd  # Data processing and manipulation, especially for handling CSV files
import tensorflow as tf  # Deep learning framework for building and training models
!pip install keras-core
!pip install keras-nlp
import keras_core as keras  # Core Keras library for creating neural networks
import keras_nlp  # Keras NLP tools for natural language processing tasks
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix  # Metrics for model evaluation
import seaborn as sns  # Data visualization library, particularly for heatmaps and plots
import matplotlib.pyplot as plt  # General plotting library for creating visualizations

# Print versions of the key libraries
print("TensorFlow version:", tf.__version__)
print("KerasNLP version:", keras_nlp.__version__)

# Load the training and test datasets using pandas
train_data = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")
sample_submission = pd.read_csv('sample_submission.csv')

print("Training data shape:", train_data.shape)
print("Test data shape:", test_data.shape)

print("Training data types:\n", train_data.dtypes)
print("\nTest data types:\n", test_data.dtypes)

train_data.head()

test_data.head()

print("Missing values in training data:\n", train_data.isnull().sum())
print("\nMissing values in test data:\n", test_data.isnull().sum())

sns.countplot(x='target', data=train_data)
plt.title('Distribution of Target Variable in Training Data')
plt.show()

print("Most common keywords in training data:", train_data['keyword'].value_counts().head(10))
print("Most common locations in training data:", train_data['location'].value_counts().head(10))

print("\nMost common keywords in test data:", test_data['keyword'].value_counts().head(10))
print("Most common locations in test data:", test_data['location'].value_counts().head(10))

# Add a length column to both DataFrames
train_data["length"] = train_data["text"].apply(lambda x: len(x) if pd.notnull(x) else 0)
test_data["length"] = test_data["text"].apply(lambda x: len(x) if pd.notnull(x) else 0)

# Print statistical summary for train and test datasets
print("Train Length Statistics")
print(train_data["length"].describe())
print()

print("Test Length Statistics")
print(test_data["length"].describe())
print()

# Optional: Visualization of the text length distribution
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.hist(train_data["length"], bins=50, color='blue', alpha=0.7)
plt.title('Train Data Text Length Distribution')
plt.xlabel('Length')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2)
plt.hist(test_data["length"], bins=50, color='green', alpha=0.7)
plt.title('Test Data Text Length Distribution')
plt.xlabel('Length')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Generate word cloud for disaster tweets
disaster_tweets = train_data[train_data['target'] == 1]['text']
disaster_text = ' '.join(disaster_tweets)
wordcloud_disaster = WordCloud(width=800, height=400, background_color='white').generate(disaster_text)

# Generate word cloud for non-disaster tweets
non_disaster_tweets = train_data[train_data['target'] == 0]['text']
non_disaster_text = ' '.join(non_disaster_tweets)
wordcloud_non_disaster = WordCloud(width=800, height=400, background_color='white').generate(non_disaster_text)

# Plot word clouds
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.imshow(wordcloud_disaster, interpolation='bilinear')
plt.title('Disaster Tweets Word Cloud')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(wordcloud_non_disaster, interpolation='bilinear')
plt.title('Non-Disaster Tweets Word Cloud')
plt.axis('off')

plt.show()

# Define columns with missing values
missing_cols = ['keyword', 'location']

# Create subplots to visualize missing values in training and test datasets
fig, axes = plt.subplots(ncols=2, figsize=(14, 6), dpi=100)

# Plot missing values for training data
sns.barplot(x=train_data[missing_cols].isnull().sum().index,
            y=train_data[missing_cols].isnull().sum().values,
            ax=axes[0],
            palette='viridis')
axes[0].set_ylabel('Missing Value Count', size=12)
axes[0].set_xlabel('Columns', size=12)
axes[0].set_title('Missing Values in Training Set', fontsize=14)
axes[0].tick_params(axis='x', labelsize=10)
axes[0].tick_params(axis='y', labelsize=10)

# Plot missing values for test data
sns.barplot(x=test_data[missing_cols].isnull().sum().index,
            y=test_data[missing_cols].isnull().sum().values,
            ax=axes[1],
            palette='viridis')
axes[1].set_ylabel('Missing Value Count', size=12)
axes[1].set_xlabel('Columns', size=12)
axes[1].set_title('Missing Values in Test Set', fontsize=14)
axes[1].tick_params(axis='x', labelsize=10)
axes[1].tick_params(axis='y', labelsize=10)

plt.tight_layout()
plt.show()

import nltk
nltk.download('stopwords')

import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Initialize objects
stop = set(stopwords.words('english'))
stemmer = PorterStemmer()
tokenizer = Tokenizer(num_words=20000, lower=True)

# Preprocessing function
def preprocess_text(text):
    if pd.isna(text):  # Handle NaN values
        return ''
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'[^\w\s]', '', text)
    text = " ".join([word for word in text.split() if word not in stop])
    text = " ".join([stemmer.stem(word) for word in text.split()])
    return text

# Apply preprocessing to train and test data
train_data['text'] = train_data['text'].apply(preprocess_text)
test_data['text'] = test_data['text'].apply(preprocess_text)

# Fit tokenizer on the processed text
tokenizer.fit_on_texts(train_data['text'].values)

# Convert text to sequences
train_sequences = tokenizer.texts_to_sequences(train_data['text'].values)
test_sequences = tokenizer.texts_to_sequences(test_data['text'].values)

# Pad sequences
max_len = 100
train_padded = pad_sequences(train_sequences, maxlen=max_len)
test_padded = pad_sequences(test_sequences, maxlen=max_len)

# Display shapes of padded sequences
print(f"Training data shape: {train_padded.shape}")
print(f"Test data shape: {test_padded.shape}")

!pip install transformers tensorflow

from sklearn.model_selection import train_test_split

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(train_padded, train_data['target'].values, test_size=0.2, random_state=42)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional

# Define model architecture
model = Sequential()
model.add(Embedding(input_dim=20000, output_dim=128, input_length=100))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Print the model summary
model.summary()

history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val), verbose=2)

val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)
print(f"Validation Loss: {val_loss}")
print(f"Validation Accuracy: {val_accuracy}")

import matplotlib.pyplot as plt

# Plot accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

sample_submission = pd.read_csv("sample_submission.csv")
sample_submission.head()

# Make predictions on the test data
test_predictions = model.predict(test_padded)
test_predictions = (test_predictions > 0.5).astype(int).flatten()

sample_submission.describe()

# Prepare the Submission File
submission = sample_submission.copy()
submission['target'] = test_predictions

# Check the first few rows of the submission file
print(submission.head())

# Save the file
submission.to_csv('submission_a4.csv', index=False)

Train_dataset='train.csv'
Test_dataset='test.csv'

Test=pd.read_csv(Test_dataset)
test_shape=Test.shape
Train=pd.read_csv(Train_dataset)
train_shape=Train.shape

test_num_rows = test_shape[0]
train_num_rows = train_shape[0]

BUFFER_SIZE = 10000
BATCH_SIZE = 64
VALIDATION_SPLIT = 0.2

main_ds = tf.data.experimental.make_csv_dataset(
    Train_dataset,
    select_columns=['text','target'],
    label_name='target',
    batch_size=BATCH_SIZE,
    num_epochs=1, # Ensure go through the data once to create a finite dataset
    shuffle=True,
    shuffle_buffer_size=BUFFER_SIZE,
    prefetch_buffer_size=tf.data.AUTOTUNE,
    num_rows_for_inference=train_num_rows # Help tf optimize the dataset creation
)

# Spilt the main dataset to train & validation datasets
num_val_elements = int(train_num_rows * VALIDATION_SPLIT // BATCH_SIZE)
raw_val_ds = main_ds.take(num_val_elements)
raw_train_ds = main_ds.skip(num_val_elements)

raw_train_ds.element_spec

for batch, label in raw_train_ds.take(1):
    for key, value in batch.items():
        print(f"{key:20s}: {value[:3]}")
    print()
    print(f"{'label':20s}: {label[:3]}")

def dict_to_text(dict, label):
  return dict['text'], label

train_ds = raw_train_ds.map(dict_to_text)
val_ds = raw_val_ds.map(dict_to_text)

from tensorflow.keras import layers

MAX_FEATURES = 10000
SEQUENCE_LENGTH = 160

vectorizer = layers.TextVectorization(
    max_tokens=MAX_FEATURES,
    output_mode='int',
    output_sequence_length=SEQUENCE_LENGTH)

# Make text-only dataset before adapt
text_ds = train_ds.map(lambda text, label:text)
vectorizer.adapt(text_ds)

vocabulary = np.array(vectorizer.get_vocabulary())
vocabulary[:20]

def vectorize_text(text, label):
  text = tf.expand_dims(text, -1)
  return vectorizer(text), label

# retrieve a batch (of 64 texts and labels) from the dataset
texts, labels = next(iter(train_ds))
text1, label1 = texts[0], labels[0]
vectorized_text, label = vectorize_text(text1, label1)
print("Original tweet: ", text1.numpy())
print("Label: ", label1.numpy())
print("Vectorized tweet: ", vectorized_text.numpy())
print("Round-trip tweet: ", " ".join(vocabulary[vectorized_text.numpy()[0]]))

model = tf.keras.Sequential([
    vectorizer,
    layers.Embedding(
        input_dim=len(vectorizer.get_vocabulary()),
        output_dim=64,
        # Use masking to handle the variable sequence lengths
        mask_zero=True),
    layers.Bidirectional(layers.LSTM(64,  return_sequences=True)),
    layers.Bidirectional(layers.LSTM(32)),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1)
])

model.summary()

print([layer.supports_masking for layer in model.layers])

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])

EPOCHS = 10
rnn_history = model.fit(
    train_ds,
    validation_data = val_ds,
    epochs = EPOCHS
)

preset= "distil_bert_base_en_uncased"

# Use a shorter sequence length.
preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,
                                                                   sequence_length=SEQUENCE_LENGTH,
                                                                   name="preprocessor_4_tweets"
                                                                  )

# Pretrained classifier.
classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,
                                                               preprocessor = preprocessor,
                                                               num_classes=2)

classifier.summary()

classifier.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(1e-4),
    metrics= ["accuracy"]
)

nlp_history = classifier.fit(train_ds,
                         epochs=10,
                         validation_data=val_ds)

